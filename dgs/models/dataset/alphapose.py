"""
Load bboxes and poses from existing .json file, generated by AP.



By default, AlphaPose saves the results for all images in one json file,
which is similar to the output box_format used by COCO.
For the basic AlphaPose output box_format see https://github.com/MVIG-SJTU/AlphaPose/blob/master/docs/output.md.

Within the .json files, AlphaPose uses the following overall structure:

image_id
    The name of the image as string. No additional path information is given.

keypoints
    The body part locations and detection confidence formatted as array ``[x1, y1, c1, x2, y2, c2, ...]``.
    Where c is the confidence score in the range [0,1] for the MPII dataset and range [0,6] for the COCO dataset.

    There can be an arbitrary number of key points, it is just known, that the length has to be divisible by three.

score
    The confidence score for the whole person, computed by AlphaPoses' parametric pose NMS.

box
    The detected bounding box as an array of floats in XYWH box_format.

idx
    The integer index of the detected person.
"""
import os

import imagesize
import torch
from torchvision import tv_tensors

from dgs.models.dataset.dataset import BaseDataset
from dgs.models.states import DataSample
from dgs.utils.files import read_json
from dgs.utils.types import Config, FilePath, NodePath, Validations

ap_load_validations: Validations = {"path": ["str", "file exists in project", ("endswith", ".json")]}


class AlphaPoseLoader(BaseDataset):
    """Load precomputed json files."""

    def __init__(self, config: Config, path: NodePath) -> None:
        super(BaseDataset, self).__init__(config=config, path=path)

        self.validate_params(ap_load_validations)

        json = read_json(self.params["path"])
        self.json: dict[str, dict]
        # different AP output formats
        if isinstance(json, list):
            self.json = {d["image_id"]: d for d in json}
        elif isinstance(json, dict):
            self.json = json
        else:
            raise NotImplementedError(f"JSON file {self.params['path']} does not contain known instances.")

        self.img_folder_path: FilePath = self.params.get("img_folder_path", "")
        self.data: list[DataSample] = self.ap_to_data_sample()

    def ap_to_data_sample(self) -> list[DataSample]:
        """Convert every detection from AlphaPose to a DataSample object.

        Yields:
            A list of DataSample objects.
        """
        samples: list[DataSample] = []

        for detection in self.json:
            keypoints, visibility = torch.split(
                tensor=torch.FloatTensor(detection["keypoints"]).reshape((-1, 3)),
                split_size_or_sections=[2, 1],
                dim=1,
            )
            file_path = os.path.join(self.img_folder_path, detection["image_id"])

            samples.append(
                DataSample(
                    filepath=file_path,
                    bbox=tv_tensors.BoundingBoxes(
                        detection["bboxes"],
                        format="XYWH",
                        canvas_size=imagesize.get(file_path),
                    ),
                    keypoints=keypoints,
                    person_id=detection["idx"] if "idx" in detection else -1,
                    # additional values which are not required
                    image_id=detection["image_id"],
                    joint_weight=visibility,
                    person_score=detection["score"],  # fixme divide by 6 for COCO, by 1 for MPII...?
                )
            )
        return samples
